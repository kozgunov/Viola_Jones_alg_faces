import cv2
import numpy as np
import time

MODEL_PATH = 'face_trained.yml' # path to the model
HAAR_CASCADE_PATH = 'haarcascade_frontalface_default.xml' # download them from my github
FEATURES_PATH = 'features.npy'  # optional
LABELS_PATH = 'labels.npy'  # optional
NAMES_ARRAY = ['person1', 'person2']  # replace with your actual attachments...

face_cascade = cv2.CascadeClassifier(HAAR_CASCADE_PATH)
face_recognizer = cv2.face.LBPHFaceRecognizer_create() # Initialize the face detector and face recognizer
face_recognizer.read(MODEL_PATH) # load the model

metadata = [] # for the report

def recognize_faces(frame):
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    faces_rect = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=4)

    for (x, y, w, h) in faces_rect: # process each detected face
        face_roi = gray[y:y + h, x:x + w]
        label, confidence = face_recognizer.predict(face_roi)
        person_name = NAMES_ARRAY[label]
        confidence_score = round(confidence, 2) # probability (0.80-1.00 means that's face detected)
        print(f'Label = {NAMES_ARRAY[label]} with a confidence of {confidence}')

        # display the name of the person & depict rectangle around them
        cv2.putText(frame, f'{NAMES_ARRAY[label]} ({round(confidence, 2)})', (x, y - 10), cv2.FONT_HERSHEY_COMPLEX, 0.65, (0, 255, 0), thickness=2)
        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), thickness=3)

        # take all possible metadata (timestamp, label, confidence, coordinates)
        metadata.append({
            'timestamp': time.strftime("%Y-%m-%d %H:%M:%S"),
            'person': person_name,
            'confidence': confidence_score,
            'position': (x, y, w, h)
        })
    return frame


def save_snapshot(frame, img_counter): # save the current frame as snapshot.
    img_name = f"snapshot_{img_counter}.png"
    cv2.imwrite(img_name, frame)
    print(f"Snapshot {img_name} saved!")

def display_report(metadata):
    print("\n--- Final Report ---")
    for entry in metadata:
        print(f"Time: {entry['timestamp']}, Person: {entry['person']}, Confidence: {entry['confidence']}%, Position: {entry['position']}")
    print("\nTotal faces detected:", len(metadata))

def inference_webcam(): # real-time (or almost real-time) face detection
    cap = cv2.VideoCapture(0)

    if not cap.isOpened(): # necessary exception handling
        print("Error: Could not open webcam.")
        return

    img_counter = 0
    processing = True
    print("Press 'p' to pause/resume, 's' to save a snapshot, 'q' to quit.")  # instruction

    while True:
        ret, frame = cap.read()
        if not ret:
            print("Error: Failed to capture image.")
            break

        # if processing is active, recognize faces
        if processing:
            result_frame = recognize_faces(frame)
        else:
            result_frame = frame.copy()

        cv2.imshow('Webcam - Recognized Faces', result_frame) # display the camera

        key = cv2.waitKey(1) & 0xFF
        if key == ord('q'): # Quit
            break
        elif key == ord('p'): # pause processing
            processing = not processing
            print(f"{'Resumed' if processing else 'Paused'} processing.")
        elif key == ord('s'): # snapshot
            img_counter += 1
            save_snapshot(result_frame, img_counter)
            display_report(metadata)

    cap.release()
    cv2.destroyAllWindows()

# start it
inference_webcam()
